{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b47690",
   "metadata": {},
   "source": [
    "# Qualtrics Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1beedbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "raw_data_filepath = '../data/survey-v0-sample-fake.csv'\n",
    "# raw_data_filepath = '../data/survey-v0-sample-raw.csv'\n",
    "CLEANED_FILEPATH = '../data/sample-preprocessed.csv'\n",
    "FIELDS_FILEPATH = '../data/fields.csv'\n",
    "\n",
    "cleaned_fields = [\n",
    "    # Fields created by Qualtrics that we *DO NOT keep*\n",
    "    #'StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress',\n",
    "    #'RecipientLastName', 'RecipientFirstName', 'RecipientEmail',\n",
    "    #'ExternalReference', 'LocationLatitude', 'LocationLongitude',\n",
    "    #'DistributionChannel', 'UserLanguage', \n",
    "    \n",
    "    # Fields created by Qualtrics that we *DO keep*\n",
    "    'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId',\n",
    "    \n",
    "    # Fields to handle the uploaded file -- do not keep\n",
    "    # 'Q43_Id', 'Q43_Name', 'Q43_Size', 'Q43_Type', \n",
    "    \n",
    "    # Fields for setup that have consent and continue vs exit Qs\n",
    "    'intro-1', 'intro-2', \n",
    "    \n",
    "    # Fields for guiding the participant through the download process\n",
    "    'download', \n",
    "    # Fields for download process failure\n",
    "    'download-fail-expl', 'download-fail-screen_Id', 'download-fail-screen_Name',\n",
    "    'download-fail-screen_Size', 'download-fail-screen_Type', \n",
    "    \n",
    "    # Fields for personal Qs. e.g. demographics data, amazon usage, life changes\n",
    "    'q-demos-age', 'Q-demos-hispanic', 'Q-demos-race', 'Q-demos-education',\n",
    "    'Q-demos-income', 'Q-demos-gender', 'Q-sexual-orientation', 'Q-demos-state', \n",
    "    'Q-amazon-use-howmany', 'Q-amazon-use-hh-size', 'Q-amazon-use-how-oft', \n",
    "    'Q-substance-use_1', 'Q-substance-use_2', 'Q-substance-use_3', \n",
    "    'Q-personal_1', 'Q-personal_2', \n",
    "    'Q-life-changes',\n",
    "    \n",
    "    # Fields for Q asking if they will share data -- specific to experiment arm\n",
    "    # 'Q-fast-completion', unused\n",
    "    'Q-control', 'Q-altruism', 'Q-bonus-05',\n",
    "    'Q-bonus-20', 'Q-bonus-50', \n",
    "    \n",
    "    # Fields for Qs about perceived data value\n",
    "    'Q-data-value-05', 'Q-data-value-20', 'Q-data-value-50', 'Q-data-value-100', \n",
    "    'Q-data-value-any', 'Q-data-value-any_1_TEXT', \n",
    "    \n",
    "    # Fields for Qs about how your data should be used\n",
    "    'Q-sell-YOUR-data', 'Q-sell-consumer-data', 'Q-small-biz-use', \n",
    "    'Q-census-use', 'Q-research-society', 'Q-attn-check',\n",
    "    \n",
    "    # Comments are not clean\n",
    "    # 'Q-comments',\n",
    "    \n",
    "    # Fields for important embedded data set set\n",
    "    # Used to indicate experiment arm:\n",
    "    'showdata',\n",
    "    'incentive', \n",
    "    # We set this to connect responses to mturk workers we pay\n",
    "    'RandomID',\n",
    "    # We set these to make the API hacks work -- do not need for analysis\n",
    "    # 'SurveyID', 'ResponseID', 'FQID', 'API_TOKEN',\n",
    "]\n",
    "\n",
    "blacklist_fields = [\n",
    "    'API_TOKEN',\n",
    "    'BATCH',\n",
    "    'DistributionChannel',\n",
    "    'EndDate',\n",
    "    'ExternalReference',\n",
    "    'FQID',\n",
    "    'Finished',\n",
    "    'IPAddress',\n",
    "    'LocationLatitude',\n",
    "    'LocationLongitude',\n",
    "    'Progress',\n",
    "    'SurveyID',\n",
    "    'ResponseID',\n",
    "    'API_TOKEN',\n",
    "    'Q-comments',\n",
    "#    'Q-fast-completion',\n",
    "    'Q43_Id',\n",
    "    'Q43_Name',\n",
    "    'Q43_Size',\n",
    "    'Q43_Type',\n",
    "    'StartDate',\n",
    "    'EndDate',\n",
    "    'Status',\n",
    "    'IPAddress',\n",
    "    'Progress',\n",
    "    'RecipientLastName',\n",
    "    'RecipientFirstName',\n",
    "    'RecipientEmail',\n",
    "    'ExternalReference',\n",
    "    'LocationLatitude',\n",
    "    'LocationLongitude',\n",
    "    'DistributionChannel',\n",
    "    'UserLanguage'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd26563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QualtricsAPI.Setup import Credentials\n",
    "survey_id = \"SV_cMiItXdF95DdF5A\"\n",
    "\n",
    "#Call the qualtrics_api_credentials() method (XM Directory Users)\n",
    "Credentials().qualtrics_api_credentials(\n",
    "    # Generate at Account / Qualtrics IDs\n",
    "    token='token',\n",
    "    # data center ID is the subdomain in the URL after SSO with mit, like:\n",
    "    # mit.co1.qualtrics.com\n",
    "    data_center='co1',\n",
    "    directory_id='directory_id') # found  on Account / Qualtrics IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f8bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from QualtricsAPI.Survey import Responses\n",
    "responses = Responses().get_survey_responses(survey=survey_id, useLabels=False)\n",
    "df = (responses\n",
    "        [responses['Q43_Id'].isna()]\n",
    "        .drop(blacklist_fields, axis=1)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875d611",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f23aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data (N=95) to ../data/sample-preprocessed.csv...\n",
      "...saved\n"
     ]
    }
   ],
   "source": [
    "print('saving data (N=%s) to %s...' % (len(df), CLEANED_FILEPATH))\n",
    "df.to_csv(CLEANED_FILEPATH, index=False)\n",
    "print('...saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2501a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cf44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from lxml import etree\n",
    "import botocore\n",
    "import boto3\n",
    "from boto.mturk.connection import MTurkConnection\n",
    "from boto.mturk.question import HTMLQuestion\n",
    "\n",
    "region_name = 'us-east-1' # modify region to match\n",
    "aws_access_key_id='key' # put your key id here\n",
    "aws_secret_access_key='secret' # put your secret key here\n",
    "\n",
    "client = boto3.client(\n",
    "    'mturk',\n",
    "#    endpoint_url=endpoint_url,\n",
    "    region_name=region_name,\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef69caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amazon_survey_HITs():\n",
    "    def delete_key(x, k):\n",
    "        \"\"\"deletes key k and returns x\"\"\"\n",
    "        del x[k]\n",
    "        return x\n",
    "    # adjust depending on what criteria our HIT needs to meet in order to be included\n",
    "    def is_survey_hit(hit): \n",
    "        \"\"\"returns True if the given hit object is a survey HIT\"\"\"\n",
    "        return 'online purchases' in hit['Title']\n",
    "    hit_pager = client.get_paginator('list_hits')\n",
    "    pages = hit_pager.paginate(PaginationConfig = {'MaxItems': 5000, 'PageSize': 100})\n",
    "    survey_HITs = []\n",
    "    for page in pages:\n",
    "        page_survey_hits = filter(is_survey_hit, page['HITs'])\n",
    "        # 'Question' is very long, it gets annoying; delete it\n",
    "        sh = map(lambda x: delete_key(x, 'Question'), page_survey_hits) \n",
    "        survey_HITs = survey_HITs + list(sh)\n",
    "    return survey_HITs\n",
    "\n",
    "survey_HITs = get_amazon_survey_HITs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41e2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing mturk API responses\n",
    "def parse_survey_answer(answer):\n",
    "    \"\"\"Needed because aws stores the answer as xml... \n",
    "    returns just the code from the XML survey answer from the hit\"\"\"\n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.fromstring(answer)\n",
    "    notags = ET.tostring(tree, encoding='unicode', method='text').replace('surveycode', '')\n",
    "    return notags\n",
    "\n",
    "# utilities for getting information about an assignment from qualtrics\n",
    "def get_bonus_amount(qualtrics_row):\n",
    "    \"\"\"Returns bonus amount ($.05, $.20, or $.50) if response was in bonus condition, $0 otherwise.\"\"\"\n",
    "    has_bonus = 'bonus' in qualtrics_row['incentive'] # if bonus not in incentive code, they don't get a bonus\n",
    "    bonus_amt = qualtrics_row['incentive'].replace('bonus-', '') if has_bonus else 0\n",
    "    return int(bonus_amt)\n",
    "\n",
    "def did_pass_attention(qualtrics_row):\n",
    "    \"\"\"returns True if they passed the attention check, false otherwise.\"\"\"\n",
    "    attn_cols = list(filter(lambda x: 'attn' in x, list(qualtrics_row.index)))\n",
    "    # they have three options. to pass they need to check all.\n",
    "    assert len(attn_cols) == 3\n",
    "    attn_answers = qualtrics_row[attn_cols].fillna(0).astype('int')\n",
    "    # if all answers are 1 (pass), sum should be 3. \n",
    "    return np.sum(attn_answers.values) == 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed8c3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothetical bonus columns\n",
    "\n",
    "def get_assignments_for_HIT(HIT_id):\n",
    "    \"\"\"returns list of assignment dicts from all iterable pages for HIT_id\"\"\"\n",
    "    all_assignments = []\n",
    "    \n",
    "    assn_pager = client.get_paginator('list_assignments_for_hit')\n",
    "    pages = assn_pager.paginate(HITId = HIT_id, \n",
    "                            AssignmentStatuses = ['Submitted'], \n",
    "                            PaginationConfig = {'MaxItems': 5000, 'PageSize': 100})\n",
    "    for page in pages:\n",
    "        for assignment in page['Assignments']:\n",
    "            all_assignments.append(assignment)\n",
    "    return all_assignments\n",
    "\n",
    "def get_worker_assignment_data(HIT_id, qualtrics_df):\n",
    "    \"\"\"returns a list of dicts representing HIT assignment data for payment.\n",
    "    \n",
    "    each dict in the list is of the form:\n",
    "     {'worker_id': str,\n",
    "      'random_id': str,\n",
    "      'bonus_amount': int | np.nan,\n",
    "      'passed_attention': bool | np.nan,\n",
    "      'found_randomID_in_qualtrics': bool},\n",
    "      \n",
    "    notes: \n",
    "    - bonus_amount and passed_attention are np.nan if we cannot link an assignment to a qualtrics response.\n",
    "    - bonus_amount is an int, so `50` corresponds to a $.50 bonus., `05` to $.05, etc.\n",
    "    \n",
    "    \"\"\"\n",
    "    assignment_results = []\n",
    "    HIT_assignments = get_assignments_for_HIT(HIT_id)\n",
    "    for assignment in HIT_assignments:\n",
    "        # get qualtrics response row\n",
    "        randomid_entered_on_hit = parse_survey_answer(assignment['Answer'])\n",
    "        worker_id = assignment['WorkerId']\n",
    "        this_assignment_data = {\n",
    "            \"worker_id\": worker_id,\n",
    "            \"random_id\": randomid_entered_on_hit\n",
    "        }\n",
    "        # dc - 10/4/22\n",
    "        # Note: some workers use their WorkerId as their RandomID in the qualtrics\n",
    "        # survey; e.g.:\n",
    "        # 'worker_id': 'A28L0Q6S2GGBJQ',\n",
    "        # 'random_id': 'A28L0Q6S2GGBJQ'\n",
    "        # no way to track what survey response that assignment connects to, so no way to check\n",
    "        # bonuses etc.\n",
    "        # the below code tries to fix for this but there are edge cases, like multiple submissions\n",
    "        # from the same worker if they enter their worker ID as their random ID each time.\n",
    "        assignment_qualtrics_row = qualtrics_df[qualtrics_df.RandomID == randomid_entered_on_hit]\n",
    "        if len(assignment_qualtrics_row) == 0:\n",
    "            # if it's 0, we didn't find the randomID in our qualtrics responses\n",
    "            this_assignment_data['found_randomID_in_qualtrics'] = False\n",
    "            this_assignment_data['bonus_amount'] = np.nan\n",
    "            this_assignment_data['passed_attention'] = np.nan\n",
    "            assignment_results.append(this_assignment_data)\n",
    "            continue\n",
    "        assert len(assignment_qualtrics_row) == 1\n",
    "        assignment_qualtrics_row = assignment_qualtrics_row.iloc[0]\n",
    "        this_assignment_data['bonus_amount'] = get_bonus_amount(assignment_qualtrics_row)\n",
    "        this_assignment_data['passed_attention'] = did_pass_attention(assignment_qualtrics_row)\n",
    "        this_assignment_data['found_randomID_in_qualtrics'] = True\n",
    "        assignment_results.append(this_assignment_data)\n",
    "    return assignment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "238ddaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIT_ID = survey_HITs[0]['HITId']\n",
    "assignment_results = get_worker_assignment_data(HIT_ID, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49c2e433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'worker_id': 'A1JI19KPIVNL3Y',\n",
       "  'random_id': '239152963',\n",
       "  'bonus_amount': 0,\n",
       "  'passed_attention': True,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'A28L0Q6S2GGBJQ',\n",
       "  'random_id': 'A28L0Q6S2GGBJQ',\n",
       "  'found_randomID_in_qualtrics': False,\n",
       "  'bonus_amount': nan,\n",
       "  'passed_attention': nan},\n",
       " {'worker_id': 'ABGKJYEITBKIL',\n",
       "  'random_id': '284052890',\n",
       "  'bonus_amount': 20,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'A9W3MMLGGWVT1',\n",
       "  'random_id': 'A9W3MMLGGWVT1',\n",
       "  'found_randomID_in_qualtrics': False,\n",
       "  'bonus_amount': nan,\n",
       "  'passed_attention': nan},\n",
       " {'worker_id': 'AQKNC6HX4QOIT',\n",
       "  'random_id': '307955165',\n",
       "  'bonus_amount': 0,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'A3Q8YKHCMJL6ZF',\n",
       "  'random_id': '284934782',\n",
       "  'bonus_amount': 50,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'A2N6XPJRQCVN5W',\n",
       "  'random_id': '380946321',\n",
       "  'bonus_amount': 50,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'A2N1LD0IFPHF4B',\n",
       "  'random_id': 'A2N1LD0IFPHF4B',\n",
       "  'found_randomID_in_qualtrics': False,\n",
       "  'bonus_amount': nan,\n",
       "  'passed_attention': nan},\n",
       " {'worker_id': 'A17LI0UQ4AAX96',\n",
       "  'random_id': '786287683',\n",
       "  'bonus_amount': 0,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'A2NWRHFNOZ51FS',\n",
       "  'random_id': '109563497',\n",
       "  'bonus_amount': 5,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'AXE3WSZONA5JG',\n",
       "  'random_id': '201530911',\n",
       "  'bonus_amount': 50,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'AANTUUK9FPBPQ',\n",
       "  'random_id': '965666540',\n",
       "  'bonus_amount': 50,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'A3V2XCDF45VN9X',\n",
       "  'random_id': '419087442',\n",
       "  'bonus_amount': 0,\n",
       "  'passed_attention': True,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'A1OFRW8QAOMRDW',\n",
       "  'random_id': 'A1OFRW8QAOMRDW',\n",
       "  'found_randomID_in_qualtrics': False,\n",
       "  'bonus_amount': nan,\n",
       "  'passed_attention': nan},\n",
       " {'worker_id': 'A237HKEFDFO4RI',\n",
       "  'random_id': '823252314',\n",
       "  'bonus_amount': 20,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'AJ0XGWHTFQ03C',\n",
       "  'random_id': '461161491',\n",
       "  'bonus_amount': 0,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'A33A2YZV7VGR2S',\n",
       "  'random_id': '769643349',\n",
       "  'bonus_amount': 5,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'AQHKPPYJJBSI9',\n",
       "  'random_id': 'AQHKPPYJJBSI9',\n",
       "  'found_randomID_in_qualtrics': False,\n",
       "  'bonus_amount': nan,\n",
       "  'passed_attention': nan},\n",
       " {'worker_id': 'A1HKMQJ3616FMY',\n",
       "  'random_id': '165743169',\n",
       "  'bonus_amount': 5,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True},\n",
       " {'worker_id': 'ARLQ8P6K4HJI6',\n",
       "  'random_id': '177998023',\n",
       "  'bonus_amount': 0,\n",
       "  'passed_attention': False,\n",
       "  'found_randomID_in_qualtrics': True}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92321034",
   "metadata": {},
   "source": [
    "Reference\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0fb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(df[df.RandomID == id1].columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
